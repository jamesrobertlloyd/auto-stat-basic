
\documentclass{article} %% For LaTeX2e
\usepackage{format/nips13submit_e}
\nipsfinalcopy %% Uncomment for camera-ready version
\usepackage{times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{color}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH}

\usepackage{graphicx, amsmath, amsfonts, bm, lipsum, capt-of}

\usepackage{natbib, xcolor, wrapfig, booktabs, multirow, caption}

\usepackage{float}

\def\ie{i.e.\ }
\def\eg{e.g.\ }

\newcommand{\home}{((( outdir )))}

\title{((* if title is defined *)) ((( title ))) ((* else *)) An automatic report for the dataset ((( data_name|escape_tex ))) ((* endif *))}

\author{
(A very basic version of) The Automatic Statistician
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\setlength{\marginparwidth}{0.9in}
\input{include/commenting.tex}

%%%% For submission, make all render blank.
%%\\renewcommand{\LATER}[1]{}
%%\\renewcommand{\\fLATER}[1]{}
%%\\renewcommand{\TBD}[1]{}
%%\\renewcommand{\\fTBD}[1]{}
%%\\renewcommand{\PROBLEM}[1]{}
%%\\renewcommand{\\fPROBLEM}[1]{}
%%\\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

\begin{document}

\allowdisplaybreaks

\maketitle

((* if body is defined *)) ((= 'body' is defined for error and server busy reports =))
((( body )))
\end{document}

((* else *))

\begin{abstract}
This is a report analysing the dataset ((( data_name|escape_tex ))).
I have compared ((( topdists|length ))) simple strategies for modelling data, using 5-fold cross validation on half of the data.
The strategy with the highest cross-validated log predictive density has then been used to train a model on the same half of data.
This model is then described, displaying the most influential components first.
Model criticism techniques have then been applied to attempt to find discrepancies between the model and data.
\end{abstract}

\section{Brief description of data set}

To confirm that I have interpreted the data correctly a short summary of the data set follows.
There are ((( n_inputs ))) input columns and ((( n_rows ))) rows of data.
A summary of these variables is given in table \ref{table:description}.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|rrr|}
\hline
 Name & Minimum & Median & Maximum \\
\hline
((* for item in summary *))
((( item.label ))) & ((( '%.2g' % item.min ))) & ((( '%.2g' % item.med ))) & ((( '%.2g' % item.max|round(2) ))) \\
((* endfor *))
\hline
\end{tabular}
\end{center}
\caption{Summary statistics of data}
\label{table:description}
\end{table}

\section{Summary of model construction}

I have compared a number of different model construction techniques by computing cross-validated log predictive density.
These figures are summarised in table \ref{table:cv-summary}.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|r|}
\hline
Method & Cross-validated log predictive density \\
\hline
((* for dist in topdists *))
((( dist.shortdescrip ))) & ((( '%.2g' % dist.avscore ))) \\
((* endfor *))
\hline
\end{tabular}
\end{center}
\caption{Summary of model construction methods and cross-validated errors}
\label{table:cv-summary}
\end{table}

((* if messages|length / topdists|length|float > 1 *)) ((= i.e. if some data doubling has happened =))
I trained the models with an increasing number of datapoints. The learning curve shows how this affects model performance and variance.
I computed the smaller datasets first, so if my analysis terminated early then some of the later models may be missing
\begin{figure}[H]
\newcommand{\mdrd}{\home/figures}
\begin{center}
\includegraphics[width=0.75\textwidth]{\mdrd/learning_curve.png}
\end{center}
\caption{Learning curve showing the effect of increasing datapoints on performance}
\end{figure}
((* endif *))

The ((( topdist.shortdescrip ))) method has the highest cross-validated log predictive density so I have used this method to train a model on half of the data.
In the rest of this report I have described this model and have attempted to criticise it using held out test data.


\section{Model description}                        


In this section I have described the model I have constructed to explain the data.
A quick summary is below, followed by quantification of the model with accompanying plots of model fit and residuals.

((* if topdist.report_id == 4 *))

\subsection{Summary}

The output ((( summary[topdist.output_index].label ))) (column ((( topdist.output_index + 1 )))):
\begin{itemize}
((* for i in topdist.input_indices *))
\item
((* if topdist.output_distribution.model.coef_[loop.index0] > 0 *))
increases((* else *))decreases((* endif *))
 linearly with input ((( summary[i].label ))) (column ((( i + 1 ))))
((* endfor *))
\end{itemize}

Here is a graphical representation:
\begin{figure}[H]
\newcommand{\mdrd}{\home/figures}
\begin{center}
\includegraphics[width=0.5\textwidth,height=0.95\textheight,keepaspectratio]{\mdrd/pgm_graph.png}
\end{center}
\caption{Directed Acyclic Graph showing the model}
\end{figure}

\subsection{Detailed plots}

((* for i in topdist.input_indices *))

\paragraph{((* if topdist.output_distribution.model.coef_[loop.index0] > 0 *)) 
Increase 
((* else *)) 
Decrease
((* endif *)) with ((( summary[i].label )))}
The correlation between the data and the input ((( summary[i].label ))) is ((( '%.2g' % topdist.output_distribution.corr[loop.index0] ))) (see figure \ref{fig:train_((( summary[i].label )))}a).

((* if (topdist.output_distribution.partcorr[loop.index0] - topdist.output_distribution.corr[loop.index0])|abs > 0.3 *))
Accounting for the rest of the model, this changes substantially to a part correlation of ((( '%.2g' % topdist.output_distribution.partcorr[loop.index0] ))) (see figure \ref{fig:train_((( summary[i].label )))}b).
((* elif (topdist.output_distribution.partcorr[loop.index0] - topdist.output_distribution.corr[loop.index0])|abs > 0.1 *))
Accounting for the rest of the model, this changes moderately to a part correlation of ((( '%.2g' % topdist.output_distribution.partcorr[loop.index0] ))) (see figure \ref{fig:train_((( summary[i].label )))}b).
((* elif (topdist.output_distribution.partcorr[loop.index0] - topdist.output_distribution.corr[loop.index0])|abs > 0.01 *))
Accounting for the rest of the model, this changes slightly to a part correlation of ((( '%.2g' % topdist.output_distribution.partcorr[loop.index0] ))) (see figure \ref{fig:train_((( summary[i].label )))}b).
((* else *))
This correlation does not change when accounting for the rest of the model (see figure \ref{fig:train_((( summary[i].label )))}b).
((* endif *))

\begin{figure}[H]
\newcommand{\wmgd}{0.5\columnwidth}
\newcommand{\mdrd}{\home/figures}
\newcommand{\mbm}{\hspace{-0.3cm}}
\begin{center}
\begin{tabular}{cc}
\mbm
\includegraphics[width=\wmgd]{\mdrd/scatter_graph_(((loop.index0))).png} &
\includegraphics[width=\wmgd]{\mdrd/partial_residuals_graph_(((loop.index0))).png} \\
a) & b) \\
\includegraphics[width=\wmgd]{\mdrd/residuals_graph_(((loop.index0))).png} &
\includegraphics[width=\wmgd]{\mdrd/histo_graph_(((loop.index0))).png} \\
c) & d) \\
\end{tabular}
\end{center}
\caption{
a) Training data plotted against input ((( summary[i].label ))).
b) Partial residuals (data minus the rest of the model) and fit of this component.
c) Residuals (data minus the full model).
d) Model for input}
\label{fig:train_((( summary[i].label )))}
\end{figure}

((* endfor *))

\section{Model criticism}
%Model is awesome and has no flaws.


((* elif topdist.report_id == 2 *)) ((= MoG section =))
\subsection{Summary}

My model for the data has ((( topdist.means|length ))) clusters.
Their sizes are shown below

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|r|}
\hline
Cluster number & Size \\
\hline
((* for item in topdist.clusterorder2ind *))
((( loop.index0 ))) & ((( topdist.clustersizes[item] ))) \\
((* endfor *))
\hline
\end{tabular}
\end{center}
\caption{Summary of cluster sizes}
\end{table}


\subsection{Detailed plots}
((* if n_inputs < 6 *))
The graph below shows all of the pairs of variables plotted against each other, with the clusters shown on top.
\begin{figure}[H]
\newcommand{\mdrd}{\home/figures}
\begin{center}
\includegraphics[width=\textwidth]{\mdrd/scatter_matrix.png}
\end{center}
\caption{Pairwise scatter plots of the data}
\end{figure}
((* endif *))

The Jensen-Shannon divergence (a measure of cluster separation) has been calculated to find projections which separate out the clusters.

\begin{figure}[H]
\newcommand{\mdrd}{\home/figures}
\begin{center}
\includegraphics[width=\textwidth]{\mdrd/js_heatmap.png}
\end{center}
\end{figure}

The four projections that best separate the clusters are shown below.
These projections have the highest values for the Jensen-Shannon divergence.
\begin{figure}[H]
\newcommand{\mdrd}{\home/figures}
\begin{center}
\includegraphics[width=\textwidth]{\mdrd/js_scatter.png}
\end{center}
\end{figure}

((* if topdist.clustersizes|length > 1 *))
Linear Discriminant Analysis has been used to find the combinations of features that maximise the linear separation.
\begin{figure}[H]
\newcommand{\mdrd}{\home/figures}
\begin{center}
\includegraphics[width=0.75\textwidth]{\mdrd/lda_scatter.png}
\end{center}
\caption{Cluster projection with LDA}
\end{figure}
((* endif *))

\section{Model criticism}
%Model is awesome and has no flaws.

((* else *))
Description not implemented!
((* endif *))


\end{document}
((* endif *))