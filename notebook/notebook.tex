\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{jmlr2e}

\usepackage{listings}
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{amssymb,amsmath}
%\usepackage{graphicx}
\usepackage{preamble}
%\usepackage{natbib}
%%%% REMEMBER ME!
%\usepackage[draft]{hyperref}
\usepackage{hyperref}
\usepackage{color}
\usepackage{url}
%\usepackage{wasysym}
%\usepackage{subfigure}
%\usepackage{tabularx}
\usepackage{booktabs}
%\usepackage{bm}
%\newcommand{\theHalgorithm}{\arabic{algorithm}}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfsubject={},
    pdfkeywords={},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH}

\setlength{\marginparwidth}{0.6in}
\input{include/commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

% Definitions of handy macros can go here

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

%\jmlrheading{Volume}{Year}{Pages}{Submitted}{Published}{James Robert Lloyd}

% Short headings should be running head and authors last names

\ShortHeadings{Auto stat basic architecture}{Lloyd et alia}
\firstpageno{1}

\begin{document}

\lstset{language=Lisp,basicstyle=\ttfamily\footnotesize} 

\title{Designing an API for the automatic statistician}

\author{\name James Robert Lloyd \email jrl44@cam.ac.uk \\
       \addr 
       Machine Learning Group \\
       Department of Engineering\\
       University of Cambridge\\
       \AND
       \name Others\dots}

\editor{Not applicable}

\maketitle

\begin{abstract}
A discussion of potential software architectures for the automatic statistician.
\end{abstract}

%\begin{keywords}
%  Gaussian processes
%\end{keywords}

\section{Introduction}

Zoubin is hoping for many students to start working on the automatic statistician project.
Current software is mostly unusable by the public.
Let's try to change that!

The plan is to design a modular system that could be called the automatic statistician.
New students can then contribute new code that satisifies certain protocols to enhance the automatic statistician.

\section{Simplifying assumptions}

We will initially make many simplifying assumptions with a view to relaxing them as we expand the basic architecture:

\begin{itemize}
  \item Data assumed to be an exchangeable sequence of vectors
  \item Only building conditional models of a single output
  \item Minimal user interaction
  \item The type of each variable (\eg binary, real valued, ordinal\dots) is known
  \item \dots
\end{itemize}

\section{Examples of desired functionality}

This describes tasks the system should be able to perform.
Some are within the scope of the current simplifying assumptions, others will require additional interfaces / messaging protocols.

\subsection{Cross validated model selection}

The data is split into folds.
For each train / validation split the training data is used to build a number of models.
The predictions of these models are then compared to the validation data and scored.
The model with the best score is then trained on the full data and returned as the selected model.

While cross validated model selection trains one model for each fold (and then another for the full data), the models trained on each fold do not need to be returned to the user.
A potential flow could be
\begin{itemize}
  \item Manager creates data set object, cross validation folds object and scoring object
  \item Manager creates several expert objects which are given access to the data, cross validation object and scoring object
  \item Experts train models (on full data) and calculate cross validated performance and store this in local knowledge bases
  \item Experts return models and cross validation performance (a tuple of label (\eg `cross validated score'), model, data set, cross validation object, method of scoring, score)
  \item Manager collates facts about cross validated performance and selects a model on the basis of these facts
  \item Manager asks the expert who created the model to describe it
\end{itemize}

\subsection{Bayesian model selection}

Much like the above, but models are scored by marginal likelihoods

\subsection{Bayesian model averaging}

Much like the above, but a Bayesian model average model object is created

\subsection{Bayesian / model-based optimisation of model selection}

This will require access to parameters of models.
A potential flow could be
\begin{itemize}
  \item Same setup as before but experts are given a small time budget before returning models and cross validated errors
  \item Manager then selects models and parameters to try based on cross validation results and model based optimisation algorithm
  \item Manager modifies the models parameters, computes cross validated errors (this will require these types of model to have `train' and `predict' procedures)
\end{itemize}

This raises the question of to what extent statisticians `own' models or whether they just create them.
I think the answer is the latter but I'm not certain - only the creator of a model would know special ways of falsifying it potentially.

Note also that `parameters' have to be high level / hyper parameters so that cross validation is appropriate here.

\subsection{Cross validated ensemble construction}

\subsection{Model criticism / checking}

After constructing / selecting a model we need to be able to attempt to falsify it.
This could be done by asking the expert who created the model (if appropriate) to perform model criticism on previously held out data.
We might also ask this to be done by a separate model criticism expert.

\section{Other desiderata}

\begin{itemize}
  \item Concurrency
  \item Language independence
  \item System independence
  \item Scalability
  \item \dots
\end{itemize}

\section{Definition of architecture}

Concurrency suggests a framework involving actors passing messages to each other.
The messages should be language independent, suggesting perhaps an XML syntax of messages?

\subsection{Overview}

Potential types of objects
\begin{itemize}
  \item Statisiticians
  \begin{itemize}
    \item Managers
    \item Model builders (experts)
    \item Criticisers / falsificationists
  \end{itemize}
  \item Data
  \begin{itemize}
    \item Cross validation splits of data
  \end{itemize}
  \item Scoring objects (\eg mean absolute error calculator)
  \item Models - these might just be very specific statisticians 
\end{itemize}

\subsection{Minimum messages}

Things like start, pause, clear, load.

Should experts have a `goal' (\eg minimise a particular score?) - and therefore this would need to be set.

\subsection{Optional messages}

Things like returning models or cross validated errors or other facts.

\subsection{Future messages}

Types of message that might be useful in future versions of the system. 

\newpage

%\appendix
%\section*{Appendix A.}
%Appendix

\vskip 0.2in
\bibliography{library}

\end{document}
